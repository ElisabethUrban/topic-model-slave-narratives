{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "This code was taken from Saric (https://github.com/SanjaSaric/HSA-Topics/blob/master/Chunking.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'C:/Users/elisa/DH-MA/data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden und sortieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_corpus = Path(data, 'fwp') # Ausgangspfad (wird nicht überschrieben)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alabama_1.txt',\n",
       " 'arkansas_1.txt',\n",
       " 'arkansas_2.txt',\n",
       " 'arkansas_3.txt',\n",
       " 'arkansas_4.txt',\n",
       " 'arkansas_5.txt',\n",
       " 'arkansas_6.txt',\n",
       " 'arkansas_7.txt',\n",
       " 'florida_1.txt',\n",
       " 'georgia_1.txt',\n",
       " 'georgia_2.txt',\n",
       " 'georgia_3.txt',\n",
       " 'georgia_4.txt',\n",
       " 'indiana_1.txt',\n",
       " 'kansas_1.txt',\n",
       " 'kentucky_1.txt',\n",
       " 'maryland_1.txt',\n",
       " 'mississippi_1.txt',\n",
       " 'missouri_1.txt',\n",
       " 'northcarolina_1.txt',\n",
       " 'northcarolina_2.txt',\n",
       " 'ohio_1.txt',\n",
       " 'oklahoma_1.txt',\n",
       " 'southcarolina_1.txt',\n",
       " 'southcarolina_2.txt',\n",
       " 'southcarolina_3.txt',\n",
       " 'southcarolina_4.txt',\n",
       " 'tennessee_1.txt',\n",
       " 'texas_1.txt',\n",
       " 'texas_2.txt',\n",
       " 'texas_3.txt',\n",
       " 'texas_4.txt',\n",
       " 'virginia_1.txt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir(path=path_to_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\alabama_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\arkansas_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\arkansas_2.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\arkansas_3.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\arkansas_4.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\arkansas_5.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\arkansas_6.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\arkansas_7.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\florida_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\georgia_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\georgia_2.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\georgia_3.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\georgia_4.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\indiana_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\kansas_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\kentucky_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\maryland_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\mississippi_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\missouri_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\northcarolina_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\northcarolina_2.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\ohio_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\oklahoma_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\southcarolina_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\southcarolina_2.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\southcarolina_3.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\southcarolina_4.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\tennessee_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\texas_1.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\texas_2.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\texas_3.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\texas_4.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\fwp\\\\virginia_1.txt']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = [os.path.join(path_to_corpus, fn) for fn in sorted(os.listdir(path_to_corpus))]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dokumente in chunks teilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(filename, n_words):\n",
    "    \"\"\"Split a text into chunks approximately `n_words` words in length.\"\"\"\n",
    "    input = open(filename, 'r', encoding=\"utf-8\")\n",
    "    words = \" \".join(re.sub(',|\\.|\\;|\\:|\\(|\\)|\\-','',input.read()).split()).split(' ') # remove special charachters and normalize space\n",
    "    input.close()\n",
    "    chunks = []\n",
    "    current_chunk_words = []\n",
    "    current_chunk_word_count = 0\n",
    "    for word in words:\n",
    "        current_chunk_words.append(word)\n",
    "        current_chunk_word_count += 1\n",
    "        if current_chunk_word_count == n_words:\n",
    "            chunks.append(' '.join(current_chunk_words))\n",
    "            current_chunk_words = []\n",
    "            current_chunk_word_count = 0\n",
    "    chunks.append(' '.join(current_chunk_words) )\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_length = 10000\n",
    "chunks = []\n",
    "\n",
    "for filename in filenames:\n",
    "    chunk_counter = 0\n",
    "    texts = split_text(filename, chunk_length)\n",
    "    for text in texts:\n",
    "        chunk = {'text': text, 'number': chunk_counter, 'filename': filename} # make dictionary with content and information\n",
    "        chunks.append(chunk)\n",
    "        chunk_counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anzahl der Originaldateien:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anzahl der erzeugten chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "272"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prüfen, ob eine Originaldatei zu kurz war, um sie im Topic Modeling überhaupt zu verwenden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl der Dateien, die du entfernen solltest:  0\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for chunk in chunks:\n",
    "    l_chunk = len(chunk['text'].split(' '))\n",
    "    if l_chunk < 3000 and chunk['number'] == 0:\n",
    "        i+=1\n",
    "        print(l_chunk, chunk['filename'],chunk['number'])\n",
    "print('Anzahl der Dateien, die du entfernen solltest: ', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hatte eine Datei beispielsweise 110 Tokens, werden 2 chunks produziert: <br>\n",
    "1) mit 100 Tokens <br>\n",
    "2) mit 10 Tokens. <br>\n",
    "Wir möchten diese kurzen chunks ihren vorhergehenden Geschwisterdateien hinzufügen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\arkansas_1.txt8 erweitert mit chunk 9 auf dem Index 22\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\arkansas_4.txt7 erweitert mit chunk 8 auf dem Index 52\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\florida_1.txt7 erweitert mit chunk 8 auf dem Index 88\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\georgia_2.txt8 erweitert mit chunk 9 auf dem Index 108\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\kentucky_1.txt3 erweitert mit chunk 4 auf dem Index 140\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\maryland_1.txt1 erweitert mit chunk 2 auf dem Index 143\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\missouri_1.txt9 erweitert mit chunk 10 auf dem Index 159\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\northcarolina_1.txt8 erweitert mit chunk 9 auf dem Index 169\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\northcarolina_2.txt7 erweitert mit chunk 8 auf dem Index 178\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\southcarolina_3.txt7 erweitert mit chunk 8 auf dem Index 223\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\tennessee_1.txt1 erweitert mit chunk 2 auf dem Index 234\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\texas_1.txt8 erweitert mit chunk 9 auf dem Index 244\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\fwp\\texas_3.txt7 erweitert mit chunk 8 auf dem Index 262\n",
      "Anzahl der erweiterten chunks: 13\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for chunk in chunks:\n",
    "    index = chunks.index(chunk)\n",
    "    l_chunk = len(chunk['text'].split(' '))\n",
    "    if l_chunk < 3000 and chunk['number'] != 0:\n",
    "        i+=1\n",
    "        chunks[index-1]['text'] = chunks[index-1]['text'] + ' ' + chunk['text']\n",
    "        print('Chunk ' + chunk['filename'] + str(chunk['number']-1) + ' erweitert mit chunk ' + str(chunk['number']) + ' auf dem Index ' + str(index))\n",
    "        \n",
    "print('Anzahl der erweiterten chunks: ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können diejenigen chunks, die bereits zu ihren Geschwisterdateien kopiert wurden, sowie diejenigen chunks, die sehr kurz waren und keine Geschwister hatten (= kurze Originalfiles) gelöscht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gelöschte chunks: 13\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for chunk in chunks:\n",
    "    index = chunks.index(chunk)\n",
    "    l_chunk = len(chunk['text'].split(' '))\n",
    "    if l_chunk < 3000:\n",
    "        i+=1\n",
    "        chunks.remove(chunk)\n",
    "        \n",
    "print('Gelöschte chunks: ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Übriggebliebene: 259\n"
     ]
    }
   ],
   "source": [
    "print('Übriggebliebene: ' + str(len(chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunks zu Textdateien speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'C:/Users/elisa/DH-MA/data/fwp-chunks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Quelle für Code: DARIAH-DE (https://liferay.de.dariah.eu/tatom/index.html)\n",
    "\n",
    "    for chunk in chunks:\n",
    "    basename = os.path.basename(chunk['filename'])\n",
    "    fn = os.path.join(output_dir, \"{}{:04d}\".format(basename, chunk['number']))\n",
    "    with open(fn, 'w', encoding='utf-8') as f:\n",
    "        f.write(chunk['text'])\n",
    "\"\"\"\n",
    "# umgeändert, sodass valide txt-Dateien als output kommen\n",
    "for chunk in chunks:\n",
    "    basename = os.path.basename(chunk['filename'])\n",
    "    fn_base, fn_ext = os.path.splitext(basename)\n",
    "    fn = os.path.join(output_dir, \"{}_{:04d}{}\".format(fn_base, chunk['number'], fn_ext))\n",
    "    with open(fn, 'w', encoding='utf-8') as f:\n",
    "        f.write(chunk['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testen, ob kurze Dateien übriggeblieben sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Übriggebliebene kurze Files:  0\n"
     ]
    }
   ],
   "source": [
    "# Test if short files remained\n",
    "i = 0\n",
    "for chunkfile in Path(data, output_dir).glob('*.txt'):\n",
    "    with open(chunkfile, encoding='utf-8') as f:\n",
    "        text = f.read().split(' ')\n",
    "        #print(len(text))\n",
    "        if len(text) < 3000:\n",
    "            i+=1\n",
    "            print(chunkfile.name)\n",
    "print('Übriggebliebene kurze Files: ', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn ja, dann sollten diese auch manuell entfernt oder zu Geschwisterdateien hinzugefügt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
