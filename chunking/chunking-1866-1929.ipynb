{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "This code was taken from Saric (https://github.com/SanjaSaric/HSA-Topics/blob/master/Chunking.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = 'C:/Users/elisa/DH-MA/data' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Laden und sortieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_corpus = Path(data, '1866-1929') # Ausgangspfad (wird nicht überschrieben)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['church-hatcher-hatcher.txt',\n",
       " 'fpn-bruce-bruce.txt',\n",
       " 'fpn-burton-burton.txt',\n",
       " 'fpn-burtont-burton.txt',\n",
       " 'fpn-ferebee-ferebee.txt',\n",
       " 'fpn-hughes-hughes.txt',\n",
       " 'fpn-lane-lane.txt',\n",
       " 'fpn-mason-mason.txt',\n",
       " 'fpn-robinson-robinson.txt',\n",
       " 'fpn-veney-veney.txt',\n",
       " 'fpn-washeducation-washing.txt',\n",
       " 'fpn-washington-washing.txt',\n",
       " 'nc-jones85-jones85.txt',\n",
       " 'nc-omarsaid-omarsaid.txt',\n",
       " 'neh-adams-adams.txt',\n",
       " 'neh-albert-albert.txt',\n",
       " 'neh-aleckson-aleckson.txt',\n",
       " 'neh-alexander-alexander.txt',\n",
       " 'neh-andersonr-andersonr.txt',\n",
       " 'neh-andersrob-andersrob.txt',\n",
       " 'neh-arter-arter.txt',\n",
       " 'neh-blair-blair.txt',\n",
       " 'neh-bleby-bleby.txt',\n",
       " 'neh-bradford-bradford.txt',\n",
       " 'neh-bragg-bragg.txt',\n",
       " 'neh-bragg1915-bragg1915.txt',\n",
       " 'neh-branham-branham.txt',\n",
       " 'neh-brown80-brown80.txt',\n",
       " 'neh-brownhal-brownhal.txt',\n",
       " 'neh-brownsn-brownsn.txt',\n",
       " 'neh-bruceje-bruceje.txt',\n",
       " 'neh-bruner-bruner.txt',\n",
       " 'neh-capehart-capehart.txt',\n",
       " 'neh-carolinatwin-carolinatwin.txt',\n",
       " 'neh-charlton-charlton.txt',\n",
       " 'neh-chesnutt-chesnutt.txt',\n",
       " 'neh-clement-clement.txt',\n",
       " 'neh-delaney-delaney.txt',\n",
       " 'neh-doug1906-doug1906.txt',\n",
       " 'neh-dougl92-dougl92.txt',\n",
       " 'neh-douglasslife-douglass.txt',\n",
       " 'neh-drumgoold-drumgoold.txt',\n",
       " 'neh-dsmith-dsmith.txt',\n",
       " 'neh-early-early.txt',\n",
       " 'neh-edwardsc-edwards.txt',\n",
       " 'neh-eliot-eliot.txt',\n",
       " 'neh-eliza2-eliza2.txt',\n",
       " 'neh-fjones-jones.txt',\n",
       " 'neh-fleming-fleming.txt',\n",
       " 'neh-flipper-flipper.txt',\n",
       " 'neh-floyd-floyd.txt',\n",
       " 'neh-foster-foster.txt',\n",
       " 'neh-franklin-franklin.txt',\n",
       " 'neh-frederick-frederick.txt',\n",
       " 'neh-gaines-gaines.txt',\n",
       " 'neh-garlick-garlick.txt',\n",
       " 'neh-greenew-greenew.txt',\n",
       " 'neh-gregory-gregory.txt',\n",
       " 'neh-hall-hall.txt',\n",
       " 'neh-harriet-harriet.txt',\n",
       " 'neh-heard-heard.txt',\n",
       " 'neh-henry-henry.txt',\n",
       " 'neh-henryg-henryg.txt',\n",
       " 'neh-henson-henson.txt',\n",
       " 'neh-henson81-henson81.txt',\n",
       " 'neh-holland-holland.txt',\n",
       " 'neh-holley-holley.txt',\n",
       " 'neh-holsey-holsey.txt',\n",
       " 'neh-iwilliams-iwilliams.txt',\n",
       " 'neh-jacksonc-jackson.txt',\n",
       " 'neh-jacksonm-jackson.txt',\n",
       " 'neh-jamesth-jamesth.txt',\n",
       " 'neh-jamison-jamison.txt',\n",
       " 'neh-jasper-jasper.txt',\n",
       " 'neh-jeter-jeter.txt',\n",
       " 'neh-johnson-johnson.txt',\n",
       " 'neh-johnson1-johnson.txt',\n",
       " 'neh-johnsontl-johnsontl.txt',\n",
       " 'neh-keckley-keckley.txt',\n",
       " 'neh-latta-latta.txt',\n",
       " 'neh-levering-levering.txt',\n",
       " 'neh-lewisj-lewisj.txt',\n",
       " 'neh-lowery-lowery.txt',\n",
       " 'neh-mallory-mallory.txt',\n",
       " 'neh-marrs-marrs.txt',\n",
       " 'neh-mars-mars.txt',\n",
       " 'neh-maysamuel-maysamuel.txt',\n",
       " 'neh-mccray-mary.txt',\n",
       " 'neh-merritt-merritt.txt',\n",
       " 'neh-mott-mott.txt',\n",
       " 'neh-natlove-natlove.txt',\n",
       " 'neh-nicholson-nicholson.txt',\n",
       " 'neh-oneal-oneal.txt',\n",
       " 'neh-parker-parker.txt',\n",
       " 'neh-pendleton-pendle.txt',\n",
       " 'neh-randolph-randolph.txt',\n",
       " 'neh-rayemma-rayemma.txt',\n",
       " 'neh-robinsonn-robinson.txt',\n",
       " 'neh-rudd-rudd.txt',\n",
       " 'neh-said-said.txt',\n",
       " 'neh-scott-scott.txt',\n",
       " 'neh-simmons-simmons.txt',\n",
       " 'neh-singleton-singleton.txt',\n",
       " 'neh-smitham-smith.txt',\n",
       " 'neh-smithhar-smithhar.txt',\n",
       " 'neh-smithj-smithj.txt',\n",
       " 'neh-straker-straker.txt',\n",
       " 'neh-stroyer-stroyer.txt',\n",
       " 'neh-stroyer85-stroyer85.txt',\n",
       " 'neh-suggs-suggs.txt',\n",
       " 'neh-taylorsu-taylorsu.txt',\n",
       " 'neh-thompsch-thompsch.txt',\n",
       " 'neh-truth75-truth75.txt',\n",
       " 'neh-truth84-truth84.txt',\n",
       " 'neh-venture2-venture2.txt',\n",
       " 'neh-walters-walters.txt',\n",
       " 'neh-washstory-washin.txt',\n",
       " 'neh-webb-webb.txt',\n",
       " 'neh-webster-webster.txt',\n",
       " 'neh-wickham-wickham.txt',\n",
       " 'neh-williams-williams.txt']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(os.listdir(path=path_to_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\church-hatcher-hatcher.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\fpn-bruce-bruce.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\fpn-burton-burton.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\fpn-burtont-burton.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\fpn-ferebee-ferebee.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\fpn-hughes-hughes.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\fpn-lane-lane.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\fpn-mason-mason.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\fpn-robinson-robinson.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\fpn-veney-veney.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\fpn-washeducation-washing.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\fpn-washington-washing.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\nc-jones85-jones85.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\nc-omarsaid-omarsaid.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-adams-adams.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-albert-albert.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-aleckson-aleckson.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-alexander-alexander.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-andersonr-andersonr.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-andersrob-andersrob.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-arter-arter.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-blair-blair.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-bleby-bleby.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-bradford-bradford.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-bragg-bragg.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-bragg1915-bragg1915.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-branham-branham.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-brown80-brown80.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-brownhal-brownhal.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-brownsn-brownsn.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-bruceje-bruceje.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-bruner-bruner.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-capehart-capehart.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-carolinatwin-carolinatwin.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-charlton-charlton.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-chesnutt-chesnutt.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-clement-clement.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-delaney-delaney.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-doug1906-doug1906.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-dougl92-dougl92.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-douglasslife-douglass.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-drumgoold-drumgoold.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-dsmith-dsmith.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-early-early.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-edwardsc-edwards.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-eliot-eliot.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-eliza2-eliza2.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-fjones-jones.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-fleming-fleming.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-flipper-flipper.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-floyd-floyd.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-foster-foster.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-franklin-franklin.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-frederick-frederick.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-gaines-gaines.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-garlick-garlick.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-greenew-greenew.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-gregory-gregory.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-hall-hall.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-harriet-harriet.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-heard-heard.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-henry-henry.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-henryg-henryg.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-henson-henson.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-henson81-henson81.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-holland-holland.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-holley-holley.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-holsey-holsey.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-iwilliams-iwilliams.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-jacksonc-jackson.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-jacksonm-jackson.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-jamesth-jamesth.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-jamison-jamison.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-jasper-jasper.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-jeter-jeter.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-johnson-johnson.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-johnson1-johnson.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-johnsontl-johnsontl.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-keckley-keckley.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-latta-latta.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-levering-levering.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-lewisj-lewisj.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-lowery-lowery.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-mallory-mallory.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-marrs-marrs.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-mars-mars.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-maysamuel-maysamuel.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-mccray-mary.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-merritt-merritt.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-mott-mott.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-natlove-natlove.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-nicholson-nicholson.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-oneal-oneal.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-parker-parker.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-pendleton-pendle.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-randolph-randolph.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-rayemma-rayemma.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-robinsonn-robinson.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-rudd-rudd.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-said-said.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-scott-scott.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-simmons-simmons.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-singleton-singleton.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-smitham-smith.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-smithhar-smithhar.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-smithj-smithj.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-straker-straker.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-stroyer-stroyer.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-stroyer85-stroyer85.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-suggs-suggs.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-taylorsu-taylorsu.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-thompsch-thompsch.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-truth75-truth75.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-truth84-truth84.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-venture2-venture2.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-walters-walters.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-washstory-washin.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-webb-webb.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-webster-webster.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-wickham-wickham.txt',\n",
       " 'C:\\\\Users\\\\elisa\\\\DH-MA\\\\data\\\\1866-1929\\\\neh-williams-williams.txt']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = [os.path.join(path_to_corpus, fn) for fn in sorted(os.listdir(path_to_corpus))]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dokumente in chunks teilen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(filename, n_words):\n",
    "    \"\"\"Split a text into chunks approximately `n_words` words in length.\"\"\"\n",
    "    input = open(filename, 'r', encoding=\"utf-8\")\n",
    "    words = \" \".join(re.sub(',|\\.|\\;|\\:|\\(|\\)|\\-','',input.read()).split()).split(' ') # remove special charachters and normalize space\n",
    "    input.close()\n",
    "    chunks = []\n",
    "    current_chunk_words = []\n",
    "    current_chunk_word_count = 0\n",
    "    for word in words:\n",
    "        current_chunk_words.append(word)\n",
    "        current_chunk_word_count += 1\n",
    "        if current_chunk_word_count == n_words:\n",
    "            chunks.append(' '.join(current_chunk_words))\n",
    "            current_chunk_words = []\n",
    "            current_chunk_word_count = 0\n",
    "    chunks.append(' '.join(current_chunk_words) )\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_length = 10000\n",
    "chunks = []\n",
    "\n",
    "for filename in filenames:\n",
    "    chunk_counter = 0\n",
    "    texts = split_text(filename, chunk_length)\n",
    "    for text in texts:\n",
    "        chunk = {'text': text, 'number': chunk_counter, 'filename': filename} # make dictionary with content and information\n",
    "        chunks.append(chunk)\n",
    "        chunk_counter += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anzahl der Originaldateien:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anzahl der erzeugten chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "580"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prüfen, ob eine Originaldatei zu kurz war, um sie im Topic Modeling überhaupt zu verwenden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1771 C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-capehart-capehart.txt 0\n",
      "Anzahl der Dateien, die du entfernen solltest:  1\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for chunk in chunks:\n",
    "    l_chunk = len(chunk['text'].split(' '))\n",
    "    if l_chunk < 3000 and chunk['number'] == 0:\n",
    "        i+=1\n",
    "        print(l_chunk, chunk['filename'],chunk['number'])\n",
    "print('Anzahl der Dateien, die du entfernen solltest: ', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hatte eine Datei beispielsweise 110 Tokens, werden 2 chunks produziert: <br>\n",
    "1) mit 100 Tokens <br>\n",
    "2) mit 10 Tokens. <br>\n",
    "Wir möchten diese kurzen chunks ihren vorhergehenden Geschwisterdateien hinzufügen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\fpn-lane-lane.txt2 erweitert mit chunk 3 auf dem Index 23\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\fpn-mason-mason.txt1 erweitert mit chunk 2 auf dem Index 26\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\fpn-veney-veney.txt0 erweitert mit chunk 1 auf dem Index 33\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\fpn-washeducation-washing.txt6 erweitert mit chunk 7 auf dem Index 41\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-adams-adams.txt0 erweitert mit chunk 1 auf dem Index 56\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-albert-albert.txt2 erweitert mit chunk 3 auf dem Index 60\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-andersonr-andersonr.txt7 erweitert mit chunk 8 auf dem Index 84\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-andersrob-andersrob.txt4 erweitert mit chunk 5 auf dem Index 90\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-bleby-bleby.txt3 erweitert mit chunk 4 auf dem Index 100\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-branham-branham.txt0 erweitert mit chunk 1 auf dem Index 110\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-brownhal-brownhal.txt6 erweitert mit chunk 7 auf dem Index 125\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-chesnutt-chesnutt.txt1 erweitert mit chunk 2 auf dem Index 137\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-clement-clement.txt1 erweitert mit chunk 2 auf dem Index 140\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-doug1906-doug1906.txt7 erweitert mit chunk 8 auf dem Index 150\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-douglasslife-douglass.txt18 erweitert mit chunk 19 auf dem Index 192\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-drumgoold-drumgoold.txt1 erweitert mit chunk 2 auf dem Index 195\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-eliot-eliot.txt1 erweitert mit chunk 2 auf dem Index 209\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-fjones-jones.txt0 erweitert mit chunk 1 auf dem Index 212\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-frederick-frederick.txt0 erweitert mit chunk 1 auf dem Index 232\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-hall-hall.txt0 erweitert mit chunk 1 auf dem Index 251\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-heard-heard.txt0 erweitert mit chunk 1 auf dem Index 256\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-henry-henry.txt1 erweitert mit chunk 2 auf dem Index 259\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-holsey-holsey.txt8 erweitert mit chunk 9 auf dem Index 299\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-jacksonc-jackson.txt3 erweitert mit chunk 4 auf dem Index 308\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-jacksonm-jackson.txt0 erweitert mit chunk 1 auf dem Index 310\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-jamison-jamison.txt4 erweitert mit chunk 5 auf dem Index 317\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-jeter-jeter.txt2 erweitert mit chunk 3 auf dem Index 325\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-johnson-johnson.txt0 erweitert mit chunk 1 auf dem Index 327\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-johnson1-johnson.txt8 erweitert mit chunk 9 auf dem Index 337\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-latta-latta.txt7 erweitert mit chunk 8 auf dem Index 356\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-mallory-mallory.txt0 erweitert mit chunk 1 auf dem Index 368\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-mars-mars.txt0 erweitert mit chunk 1 auf dem Index 374\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-maysamuel-maysamuel.txt1 erweitert mit chunk 2 auf dem Index 377\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-mccray-mary.txt2 erweitert mit chunk 3 auf dem Index 381\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-oneal-oneal.txt0 erweitert mit chunk 1 auf dem Index 402\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-rayemma-rayemma.txt8 erweitert mit chunk 9 auf dem Index 426\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-robinsonn-robinson.txt2 erweitert mit chunk 3 auf dem Index 430\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-smitham-smith.txt20 erweitert mit chunk 21 auf dem Index 505\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-stroyer-stroyer.txt0 erweitert mit chunk 1 auf dem Index 518\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-thompsch-thompsch.txt1 erweitert mit chunk 2 auf dem Index 528\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-truth84-truth84.txt8 erweitert mit chunk 9 auf dem Index 547\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-webb-webb.txt2 erweitert mit chunk 3 auf dem Index 569\n",
      "Chunk C:\\Users\\elisa\\DH-MA\\data\\1866-1929\\neh-williams-williams.txt3 erweitert mit chunk 4 auf dem Index 579\n",
      "Anzahl der erweiterten chunks: 43\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for chunk in chunks:\n",
    "    index = chunks.index(chunk)\n",
    "    l_chunk = len(chunk['text'].split(' '))\n",
    "    if l_chunk < 3000 and chunk['number'] != 0:\n",
    "        i+=1\n",
    "        chunks[index-1]['text'] = chunks[index-1]['text'] + ' ' + chunk['text']\n",
    "        print('Chunk ' + chunk['filename'] + str(chunk['number']-1) + ' erweitert mit chunk ' + str(chunk['number']) + ' auf dem Index ' + str(index))\n",
    "        \n",
    "print('Anzahl der erweiterten chunks: ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können diejenigen chunks, die bereits zu ihren Geschwisterdateien kopiert wurden, sowie diejenigen chunks, die sehr kurz waren und keine Geschwister hatten (= kurze Originalfiles) gelöscht werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gelöschte chunks: 44\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for chunk in chunks:\n",
    "    index = chunks.index(chunk)\n",
    "    l_chunk = len(chunk['text'].split(' '))\n",
    "    if l_chunk < 3000:\n",
    "        i+=1\n",
    "        chunks.remove(chunk)\n",
    "        \n",
    "print('Gelöschte chunks: ' + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Übriggebliebene: 536\n"
     ]
    }
   ],
   "source": [
    "print('Übriggebliebene: ' + str(len(chunks)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chunks zu Textdateien speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'C:/Users/elisa/DH-MA/data/1866-1929-chunks'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Quelle für Code: DARIAH-DE (https://liferay.de.dariah.eu/tatom/index.html)\n",
    "\n",
    "    for chunk in chunks:\n",
    "    basename = os.path.basename(chunk['filename'])\n",
    "    fn = os.path.join(output_dir, \"{}{:04d}\".format(basename, chunk['number']))\n",
    "    with open(fn, 'w', encoding='utf-8') as f:\n",
    "        f.write(chunk['text'])\n",
    "\"\"\"\n",
    "# umgeändert, sodass valide txt-Dateien als output kommen\n",
    "for chunk in chunks:\n",
    "    basename = os.path.basename(chunk['filename'])\n",
    "    fn_base, fn_ext = os.path.splitext(basename)\n",
    "    fn = os.path.join(output_dir, \"{}_{:04d}{}\".format(fn_base, chunk['number'], fn_ext))\n",
    "    with open(fn, 'w', encoding='utf-8') as f:\n",
    "        f.write(chunk['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testen, ob kurze Dateien übriggeblieben sind:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Übriggebliebene kurze Files:  0\n"
     ]
    }
   ],
   "source": [
    "# Test if short files remained\n",
    "i = 0\n",
    "for chunkfile in Path(data, output_dir).glob('*.txt'):\n",
    "    with open(chunkfile, encoding='utf-8') as f:\n",
    "        text = f.read().split(' ')\n",
    "        #print(len(text))\n",
    "        if len(text) < 3000:\n",
    "            i+=1\n",
    "            print(chunkfile.name)\n",
    "print('Übriggebliebene kurze Files: ', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn ja, dann sollten diese auch manuell entfernt oder zu Geschwisterdateien hinzugefügt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
